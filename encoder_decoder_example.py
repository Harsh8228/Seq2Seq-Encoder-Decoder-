# -*- coding: utf-8 -*-
"""Encoder decoder example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hCQf06Rpe_YdK4n9YlTC-Mav6gk4OtII
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Prepare a Tiny Translation Dataset

# English sentences (source) and French sentences (target)
english_sentences = ["hello", "how are you", "i am fine", "good morning", "good night", "see you later", "thank you", "yes", "no", "i love you"]

french_sentences = ["bonjour", "comment ça va", "je vais bien", "bonjour", "bonne nuit", "à plus tard", "merci", "oui", "non", "je t'aime" ]

# For the target (French) sentences, add special tokens for start and end of sentence becasue decoder works token by token but encoder generates
#context vector so it doesnt need sos snd eos.
target_sentences = ["<sos> " + sent + " <eos>" for sent in french_sentences]

# Tokenize and Pad Sequences

# Tokenizer for English (source)
eng_tokenizer = Tokenizer(filters='')
eng_tokenizer.fit_on_texts(english_sentences)
input_sequences = eng_tokenizer.texts_to_sequences(english_sentences)
max_encoder_seq_length = max(len(seq) for seq in input_sequences)
encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')

# Tokenizer for French (target)
fr_tokenizer = Tokenizer(filters='')
fr_tokenizer.fit_on_texts(target_sentences)
target_sequences = fr_tokenizer.texts_to_sequences(target_sentences)
max_decoder_seq_length = max(len(seq) for seq in target_sequences)
decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')

# For training, the target output is the decoder input shifted by one.
decoder_target_data = np.zeros_like(decoder_input_data)
decoder_target_data[:, :-1] = decoder_input_data[:, 1:]
decoder_target_data[:, -1] = 0  # last token as padding

num_encoder_tokens = len(eng_tokenizer.word_index) + 1
num_decoder_tokens = len(fr_tokenizer.word_index) + 1

print("English Vocabulary Size:", num_encoder_tokens)
print("French Vocabulary Size:", num_decoder_tokens)
print("Max encoder sequence length:", max_encoder_seq_length)
print("Max decoder sequence length:", max_decoder_seq_length)

# Build the Encoder-Decoder Model

latent_dim = 256  # Latent dimensionality of the encoding space

# --- Encoder ---
encoder_inputs = Input(shape=(None,), name='encoder_inputs')
encoder_embedding_layer = Embedding(input_dim=num_encoder_tokens, output_dim=latent_dim, name='encoder_embedding')
encoder_embedding = encoder_embedding_layer(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')
_, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# --- Decoder ---
decoder_inputs = Input(shape=(None,), name='decoder_inputs')
decoder_embedding_layer = Embedding(input_dim=num_decoder_tokens, output_dim=latent_dim, name='decoder_embedding')
decoder_embedding = decoder_embedding_layer(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')
decoder_outputs = decoder_dense(decoder_outputs)

# Full training model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model (with a small number of epochs for demonstration)
model.fit([encoder_input_data, decoder_input_data],
          np.expand_dims(decoder_target_data, -1),
          batch_size=16, epochs=100, verbose=2)

# Build Inference Models

# Encoder model: same as training encoder.
encoder_model = Model(encoder_inputs, encoder_states)

# Decoder model for inference:
decoder_state_input_h = Input(shape=(latent_dim,), name='input_h')
decoder_state_input_c = Input(shape=(latent_dim,), name='input_c')
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

# Use the same embedding layer for inference.
dec_emb2 = decoder_embedding_layer(decoder_inputs)
decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)
decoder_states2 = [state_h2, state_c2]
decoder_outputs2 = decoder_dense(decoder_outputs2)
decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)

# Define the Decoding Function (Inference)

def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq)

    # Generate the start token index for French.
    start_token_index = fr_tokenizer.word_index['<sos>']
    end_token_index = fr_tokenizer.word_index['<eos>']

    target_seq = np.array([[start_token_index]])
    decoded_sentence = []

    while True:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        # Get the token with the highest probability.
        sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))
        # Stop if the end token is predicted.
        if sampled_token_index == end_token_index or len(decoded_sentence) > max_decoder_seq_length:
            break
        decoded_sentence.append(sampled_token_index)

        # Update the target sequence for the next timestep.
        target_seq = np.array([[sampled_token_index]])
        states_value = [h, c]

    # Convert token indexes to words.
    reverse_fr_word_index = {v: k for k, v in fr_tokenizer.word_index.items()}
    decoded_words = [reverse_fr_word_index.get(i, '') for i in decoded_sentence]
    return ' '.join(decoded_words)

# Demonstrate with an Example

# Choose an example English sentence
example_sentence = "see you later"
print("Input (English):", example_sentence)

# Convert the input sentence into sequence format.
example_seq = eng_tokenizer.texts_to_sequences([example_sentence])
example_seq = pad_sequences(example_seq, maxlen=max_encoder_seq_length, padding='post')

# Decode the sequence.
translated_sentence = decode_sequence(example_seq)
print("Predicted Translation (French):", translated_sentence)

# For comparison, print the actual target sentence for that example.
# (Find the corresponding target sentence from our tiny dataset.)
index = english_sentences.index(example_sentence)
print("Actual French Sentence:", french_sentences[index])